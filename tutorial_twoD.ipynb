{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch as t\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision as tv\n",
    "# import torchvision.datasets as dset\n",
    "# import torchvision.transforms as transforms\n",
    "# import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manualSeed = 42\n",
    "print(\"Random seed = \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "t.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = './data/celeba'\n",
    "num_workers = 2\n",
    "batch_size=64\n",
    "image_size=64\n",
    "\n",
    "nc = 3 #no. channels of images\n",
    "nz = 100\n",
    "ngf = 64 #no. generator's feature maps\n",
    "ndf = 64 #no. discriminator's feature maps\n",
    "num_epochs = 8\n",
    "lr = .0002\n",
    "beta1 = .5\n",
    "ngpu = 1 #no. gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "dataset = tv.datasets.ImageFolder(root=dataroot,\n",
    "                                 transform=tv.transforms.Compose([\n",
    "                                     tv.transforms.Resize(image_size), \n",
    "                                     tv.transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                     tv.transforms.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2)),\\\n",
    "                                     tv.transforms.RandomAffine(0, scale=(.95, 1.2)), \\\n",
    "                                     tv.transforms.CenterCrop(image_size),\n",
    "                                     tv.transforms.ToTensor(),\n",
    "                                     tv.transforms.Normalize((.5, .5, .5), (.5, .5, .5)), \n",
    "                                     AddGaussianNoise(.5, .05),\n",
    "                                 ]))\n",
    "dataset = tv.datasets.ImageFolder(root=dataroot,\n",
    "                           transform=tv.transforms.Compose([\n",
    "                               tv.transforms.Resize(image_size),\n",
    "                               tv.transforms.CenterCrop(image_size),\n",
    "                               tv.transforms.ToTensor(),\n",
    "                               tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "dataloader = t.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "device = t.device(\"cuda:0\" if (t.cuda.is_available() and ngpu > 0) else 'cpu')\n",
    "\n",
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.axis('off')\n",
    "plt.title('training images')\n",
    "plt.imshow(np.transpose(tv.utils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(), (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, init model weight according to normal distribution, why?\n",
    "def weight_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        t.nn.init.normal_(m.weight.data, .0, .02)\n",
    "    elif classname.find('BatchNorm')!= -1:\n",
    "        t.nn.init.normal_(m.weight.data, 1.0, .02)\n",
    "        t.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(t.nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.conv1 = t.nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False)\n",
    "        self.bn1 = t.nn.BatchNorm2d(ngf * 8)\n",
    "        self.conv2 = t.nn.ConvTranspose2d( ngf * 8, ngf * 4, 4, 2, 1, bias=False)\n",
    "        self.bn2 = t.nn.BatchNorm2d(ngf * 4)\n",
    "        self.conv3 = t.nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False)\n",
    "        self.bn3 = t.nn.BatchNorm2d(ngf * 2)\n",
    "        self.conv4 = t.nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False)\n",
    "        self.bn4 = t.nn.BatchNorm2d(ngf * 1)\n",
    "        self.conv5 = t.nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = t.nn.functional.leaky_relu(self.bn1(self.conv1(x)), negative_slope=0.2)\n",
    "        x = t.nn.functional.leaky_relu(self.bn2(self.conv2(x)), negative_slope=0.2)\n",
    "        x = t.nn.functional.leaky_relu(self.bn3(self.conv3(x)), negative_slope=0.2)\n",
    "        x = t.nn.functional.leaky_relu(self.bn4(self.conv4(x)), negative_slope=0.2)\n",
    "        x = t.tanh(self.conv5(x))\n",
    "        return x\n",
    "\n",
    "class GeneratorV0(t.nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = t.nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            t.nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            t.nn.BatchNorm2d(ngf * 8),\n",
    "            t.nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            t.nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            t.nn.BatchNorm2d(ngf * 4),\n",
    "            t.nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            t.nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            t.nn.BatchNorm2d(ngf * 2),\n",
    "            t.nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            t.nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            t.nn.BatchNorm2d(ngf),\n",
    "            t.nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            t.nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            t.nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netg = Generator(ngpu).to(device)\n",
    "if (device.type=='cuda') and (ngpu > 1):\n",
    "    netg = t.nn.DataParallel(netg, list(range(ngpu)))\n",
    "netg.apply(weight_init)  #apply weight_init recursively to all children\n",
    "print(netg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in netg.children():\n",
    "    for item2 in item.children():\n",
    "        print (item2.__class__.__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(t.nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.conv1 = t.nn.Conv2d(nc, ndf, 4, 2, 1, bias=False)\n",
    "        self.bn1 = t.nn.BatchNorm2d(ndf * 1)\n",
    "\n",
    "        self.conv2 = t.nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False)\n",
    "        self.bn2 = t.nn.BatchNorm2d(ndf * 2)\n",
    "        self.conv3 = t.nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False)\n",
    "        self.bn3 = t.nn.BatchNorm2d(ndf * 4)\n",
    "        self.conv4 = t.nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False)\n",
    "        self.bn4 = t.nn.BatchNorm2d(ndf * 8)\n",
    "        self.conv5 = t.nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = t.nn.functional.leaky_relu(self.bn1(self.conv1(x)), negative_slope=0.2)\n",
    "        x = t.nn.functional.leaky_relu(self.bn2(self.conv2(x)), negative_slope=0.2)\n",
    "        x = t.nn.functional.leaky_relu(self.bn3(self.conv3(x)), negative_slope=0.2)\n",
    "        x = t.nn.functional.leaky_relu(self.bn4(self.conv4(x)), negative_slope=0.2)\n",
    "        x = t.sigmoid(self.conv5(x))\n",
    "        return x\n",
    "\n",
    "class DiscriminatorV0(t.nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = t.nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            t.nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            t.nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            t.nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            t.nn.BatchNorm2d(ndf * 2),\n",
    "            t.nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            t.nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            t.nn.BatchNorm2d(ndf * 4),\n",
    "            t.nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            t.nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            t.nn.BatchNorm2d(ndf * 8),\n",
    "            t.nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            t.nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            t.nn.S ()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Descriminator No 1\")\n",
    "netd1 = Discriminator(ngpu).to(device)\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netd1 = t.nn.DataParallel(netd1, list(range(ngpu)))\n",
    "netd1.apply(weight_init)\n",
    "print(netd1)\n",
    "\n",
    "print (\"Descriminator No 2\")\n",
    "netd2 = Discriminator(ngpu).to(device)\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netd2 = t.nn.DataParallel(netd2, list(range(ngpu)))\n",
    "netd2.apply(weight_init)\n",
    "print(netd2)\n",
    "\n",
    "print (\"Descriminator No 3\")\n",
    "netd3 = Discriminator(ngpu).to(device)\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netd3 = t.nn.DataParallel(netd3, list(range(ngpu)))\n",
    "netd3.apply(weight_init)\n",
    "print(netd3)\n",
    "\n",
    "print (\"Descriminator No 4\")\n",
    "netd4 = Discriminator(ngpu).to(device)\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netd4 = t.nn.DataParallel(netd4, list(range(ngpu)))\n",
    "netd4.apply(weight_init)\n",
    "print(netd4)\n",
    "\n",
    "print (\"Descriminator No 5\")\n",
    "netd5 = Discriminator(ngpu).to(device)\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netd5 = t.nn.DataParallel(netd5, list(range(ngpu)))\n",
    "netd5.apply(weight_init)\n",
    "print(netd5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = t.nn.BCELoss()\n",
    "#criterion = t.nn.BCEWithLogitsLoss()\n",
    "\n",
    "fixed_noise = t.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "label_real = 1\n",
    "label_fake = 0\n",
    "\n",
    "optimizer_d1 = optim.Adam(netd1.parameters(), lr=lr, betas=(beta1, .999))\n",
    "optimizer_d2 = optim.Adam(netd2.parameters(), lr=lr*.1, betas=(beta1, .999))\n",
    "optimizer_d3 = optim.Adam(netd3.parameters(), lr=lr*10, betas=(beta1, .999))\n",
    "optimizer_d4 = optim.RMSprop(netd3.parameters(), lr=lr*10, betas=(beta1, .999))\n",
    "optimizer_d5 = optim.SGD(netd3.parameters(), lr=lr, momentum=0.9)\n",
    "optimizer_g = optim.Adam(netg.parameters(), lr=lr, betas=(beta1, .999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def iter_d(data, net_d, net_g, optimizer_d, optimizer_g, noise=None):\n",
    "    net_d.zero_grad()\n",
    "    real_cpu_ = data[0].to(device)\n",
    "    b_size_ = real_cpu_.size(0)\n",
    "    \n",
    "    if noise is not None:\n",
    "        noise = t.randn(b_size_, nz, 1, 1, device=device)\n",
    "    noise_ = noise\n",
    "    \n",
    "    label_ = t.full((b_size_,), label_real, dtype=t.float, device=device) #create a vector [label_real, ...] equal to batch size\n",
    "    output_ = net_d(real_cpu_).view(-1)\n",
    "    err_d_real_ = criterion(output_, label_)\n",
    "    err_d_real_.backward()\n",
    "    D_x_ = output_.mean().item() #why mean here? mean over all batch values?\n",
    "    \n",
    "    fake_ = net_g(noise_)\n",
    "    label_.fill_(label_fake) #change the gt label, no be [label_fake, ...] instead\n",
    "    output_ = net_d(fake_.detach()).view(-1)\n",
    "    err_d_fake_ = criterion(output_, label_)\n",
    "    err_d_fake_.backward()\n",
    "    D_G_z1_ = output_.mean().item() #D(G(z)) over batch of z\n",
    "    err_d_ = err_d_real_ + err_d_fake_\n",
    " \n",
    "    optimizer_d.step()\n",
    "    \n",
    "    net_g.zero_grad()\n",
    "    label_.fill_(label_real)\n",
    "    output_ = net_d(fake_).view(-1)\n",
    "    err_g_ = criterion(output_, label_)\n",
    "    err_g_.backward()\n",
    "    D_G_z2_ = output_.mean().item()\n",
    "        \n",
    "    optimizer_g.step()  \n",
    "    \n",
    "    return D_x_, D_G_z1_, D_G_z2_, err_d_, err_g_\n",
    "    \n",
    "\n",
    "img_list = []\n",
    "g_loss = []\n",
    "d_loss = []\n",
    "iters = 0\n",
    "\n",
    "print ('Start training')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        noise = t.randn(b_size, nz, 1, 1, device=device)\n",
    "        D_x, D_G_z1, D_G_z2, err_d, err_g = iter_d(data, netd1, netg, optimizer_d1, optimizer_g, noise=noise)\n",
    "        iter_d(data, netd2, netg, optimizer_d2, optimizer_g, noise=noise)\n",
    "        iter_d(data, netd3, netg, optimizer_d2, optimizer_g, noise=noise)\n",
    "        \n",
    "        if i%50 == 0:\n",
    "            print (f'[{epoch}/{num_epochs}][{i}/{len(dataloader)}] \\t Loss_D: {err_d.item():.4f}, Loss_G: {err_g.item():.4f} \\t D(x): {D_x} \\t D(G(z)): {D_G_z1:.4f} / {D_G_z2:.4f}')\n",
    "        \n",
    "        g_loss.append(err_g.item())\n",
    "        d_loss.append(err_d.item())\n",
    "        \n",
    "        if (iters % 1000 == 0) or ((epoch == num_epochs - 1) and (i == len(dataloader) - 1)):\n",
    "            with t.no_grad():\n",
    "                fake = netg(fixed_noise).detach().cpu()\n",
    "            img_list.append(tv.utils.make_grid(fake, padding=2, normalize=True))    \n",
    "            \n",
    "        iters += 1#fine grain iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('generator & discriminator loss')\n",
    "plt.plot(g_loss, label='G')\n",
    "plt.plot(d_loss, label='D')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.axis('off')\n",
    "plt.title('real images')\n",
    "plt.imshow(np.transpose(tv.utils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(), (1, 2, 0)))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.axis('off')\n",
    "plt.title('fake images')\n",
    "plt.imshow(np.transpose(img_list[-1], (1, 2, 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape, label.shape, real_cpu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_pytorch_python3)",
   "language": "python",
   "name": "env_pytorch_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
