{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch as t\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision as tv\n",
    "# import torchvision.datasets as dset\n",
    "# import torchvision.transforms as transforms\n",
    "# import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manualSeed = 42\n",
    "print(\"Random seed = \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "t.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = './data/celeba'\n",
    "num_workers = 2\n",
    "batch_size=64\n",
    "image_size=64\n",
    "\n",
    "nc = 3 #no. channels of images\n",
    "nz = 100\n",
    "ngf = 64 #no. generator's feature maps\n",
    "ndf = 64 #no. discriminator's feature maps\n",
    "num_epochs = 5\n",
    "lr = .0002\n",
    "beta1 = .5\n",
    "ngpu = 1 #no. gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "dataset = tv.datasets.ImageFolder(root=dataroot,\n",
    "                                 transform=tv.transforms.Compose([\n",
    "                                     tv.transforms.Resize(image_size), \n",
    "                                     tv.transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                     tv.transforms.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2)),\\\n",
    "                                     tv.transforms.RandomAffine(0, scale=(.95, 1.2)), \\\n",
    "                                     tv.transforms.CenterCrop(image_size),\n",
    "                                     tv.transforms.ToTensor(),\n",
    "                                     tv.transforms.Normalize((.5, .5, .5), (.5, .5, .5)), \n",
    "                                     AddGaussianNoise(.5, .05),\n",
    "                                 ]))\n",
    "dataset = tv.datasets.ImageFolder(root=dataroot,\n",
    "                           transform=tv.transforms.Compose([\n",
    "                               tv.transforms.Resize(image_size),\n",
    "                               tv.transforms.CenterCrop(image_size),\n",
    "                               tv.transforms.ToTensor(),\n",
    "                               tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "dataloader = t.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "device = t.device(\"cuda:0\" if (t.cuda.is_available() and ngpu > 0) else 'cpu')\n",
    "\n",
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.axis('off')\n",
    "plt.title('training images')\n",
    "plt.imshow(np.transpose(tv.utils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(), (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, init model weight according to normal distribution, why?\n",
    "def weight_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        t.nn.init.normal_(m.weight.data, .0, .02)\n",
    "    elif classname.find('BatchNorm')!= -1:\n",
    "        t.nn.init.normal_(m.weight.data, 1.0, .02)\n",
    "        t.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(t.nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.fc1 = t.nn.Linear(nz, nc * image_size * image_size)\n",
    "        \n",
    "        self.conv1 = t.nn.Conv2d( nc, ngf * 2, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = t.nn.BatchNorm2d(ngf * 2)\n",
    "        self.conv2 = t.nn.Conv2d( ngf * 2, ngf * 4, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = t.nn.BatchNorm2d(ngf * 4)\n",
    "        self.conv3 = t.nn.Conv2d( ngf * 4, ngf * 8, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = t.nn.BatchNorm2d(ngf * 8)\n",
    "        self.conv4 = t.nn.ConvTranspose2d( ngf * 8, ngf * 4, 4, 2, 1, bias=False)\n",
    "        self.bn4 = t.nn.BatchNorm2d(ngf * 4)\n",
    "        self.conv5 = t.nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False)\n",
    "        self.bn5 = t.nn.BatchNorm2d(ngf * 2)\n",
    "        self.conv6 = t.nn.ConvTranspose2d( ngf * 2, nc, 4, 2, 1, bias=False)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.shape[1] == nz: #if input is a latent vector, convert it into an image\n",
    "            x = t.nn.functional.leaky_relu(self.fc1(x.squeeze()).view(-1, nc, image_size, image_size), negative_slope=0.2)\n",
    "        x = t.nn.functional.leaky_relu(self.bn1(self.conv1(x)), negative_slope=0.2)\n",
    "        #print (x.shape)\n",
    "        x = t.nn.functional.leaky_relu(self.bn2(self.conv2(x)), negative_slope=0.2)\n",
    "        #print (x.shape)\n",
    "        x = t.nn.functional.leaky_relu(self.bn3(self.conv3(x)), negative_slope=0.2)\n",
    "        #print (x.shape)\n",
    "        x = t.nn.functional.leaky_relu(self.bn4(self.conv4(x)), negative_slope=0.2)\n",
    "        #print (x.shape)\n",
    "        x = t.nn.functional.leaky_relu(self.bn5(self.conv5(x)), negative_slope=0.2)\n",
    "        #print (x.shape)        \n",
    "        x = t.tanh(self.conv6(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GeneratorV1(t.nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        #THE CODE BELOW IS MESSED UP, NEED TO BE RECOVERED\n",
    "        \n",
    "        self.conv1 = t.nn.Conv2d( nc, ngf * 1, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = t.nn.BatchNorm2d(ngf * 1)\n",
    "        self.conv2 = t.nn.Conv2d( ngf * 1, ngf * 2, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = t.nn.BatchNorm2d(ngf * 2)\n",
    "        self.conv3 = t.nn.Conv2d( ngf * 2, ngf * 4, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = t.nn.BatchNorm2d(ngf * 4)\n",
    "        #self.conv4 = t.nn.Conv2d( ngf * 4, ngf * 8, 4, 2, 1, bias=False)\n",
    "        #self.bn4 = t.nn.BatchNorm2d(ngf * 8)\n",
    "        self.conv4 = t.nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False)\n",
    "        self.bn4 = t.nn.BatchNorm2d(ngf * 2)        \n",
    "        self.conv5 = t.nn.ConvTranspose2d( ngf * 2, ngf * 1, 4, 2, 1, bias=False)\n",
    "        self.bn5 = t.nn.BatchNorm2d(ngf * 1)\n",
    "        self.conv6 = t.nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = t.nn.functional.leaky_relu(self.bn1(self.conv1(x)), negative_slope=0.2)\n",
    "        x = t.nn.functional.leaky_relu(self.bn2(self.conv2(x)), negative_slope=0.2)\n",
    "        x = t.nn.functional.leaky_relu(self.bn3(self.conv3(x)), negative_slope=0.2)\n",
    "        x = t.nn.functional.leaky_relu(self.bn4(self.conv4(x)), negative_slope=0.2)\n",
    "        x = t.nn.functional.leaky_relu(self.bn5(self.conv5(x)), negative_slope=0.2)\n",
    "        x = t.tanh(self.conv6(x))\n",
    "        return x\n",
    "\n",
    "class GeneratorV0(t.nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = t.nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            t.nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            t.nn.BatchNorm2d(ngf * 8),\n",
    "            t.nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            t.nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            t.nn.BatchNorm2d(ngf * 4),\n",
    "            t.nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            t.nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            t.nn.BatchNorm2d(ngf * 2),\n",
    "            t.nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            t.nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            t.nn.BatchNorm2d(ngf),\n",
    "            t.nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            t.nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            t.nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# netg = Generator(ngpu).to(device)\n",
    "# if (device.type=='cuda') and (ngpu > 1):\n",
    "#     netg = t.nn.DataParallel(netg, list(range(ngpu)))\n",
    "# netg.apply(weight_init)  #apply weight_init recursively to all children\n",
    "# print(netg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in netg.children():\n",
    "#     for item2 in item.children():\n",
    "#         print (item2.__class__.__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE_D = 'D'\n",
    "MODE_G = 'G'\n",
    "class Discriminator(t.nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.fc1a = t.nn.Linear(nz, nc * image_size * image_size)\n",
    "        self.conv1a = t.nn.Conv2d( nc, ngf * 2, 3, stride=1, padding=1, bias=False)\n",
    "        self.bn1a = t.nn.BatchNorm2d(ngf * 2)\n",
    "        self.conv1b = t.nn.Conv2d( nc, ngf * 2, 3, stride=1, padding=1, bias=False)\n",
    "        self.bn1b = t.nn.BatchNorm2d(ngf * 2)\n",
    "\n",
    "        \n",
    "        self.conv2a = t.nn.Conv2d(ngf * 2, ngf * 2, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn2a = t.nn.BatchNorm2d(ngf * 2)\n",
    "        self.conv2b = t.nn.Conv2d(ngf * 2, ngf * 2, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn2b = t.nn.BatchNorm2d(ngf * 2)\n",
    "        \n",
    "        self.conv3a = t.nn.Conv2d( ngf * 2, ngf * 4, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn3a = t.nn.BatchNorm2d(ngf * 4)\n",
    "        self.conv3b = t.nn.Conv2d( ngf * 2, ngf * 4, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn3b = t.nn.BatchNorm2d(ngf * 4)        \n",
    "        \n",
    "        self.conv4 = t.nn.Conv2d( ngf * 4, ngf * 8, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn4 = t.nn.BatchNorm2d(ngf * 8)\n",
    "        \n",
    "        self.conv5 = t.nn.ConvTranspose2d( ngf * 8, ngf * 4, 4, 2, 1, bias=False)\n",
    "        self.bn5 = t.nn.BatchNorm2d(ngf * 4)\n",
    "        self.conv6 = t.nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False)\n",
    "        self.bn6 = t.nn.BatchNorm2d(ngf * 2)\n",
    "        self.conv7 = t.nn.ConvTranspose2d( ngf * 2, nc, 4, 2, 1, bias=False)\n",
    "        \n",
    "        self.aapool2 = t.nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc2 = t.nn.Linear(ngf * 2, 1024)\n",
    "        self.fc3 = t.nn.Linear(1024, 1)\n",
    "    \n",
    "    def forward(self, x, z, mode):\n",
    "        #TODO: \n",
    "        #print (x.shape)\n",
    "#         if mode==MODE_G: #input mode is generator, work on latent vector z\n",
    "#             #x = t.zero_(x)#, device=device)\n",
    "#             x = t.zeros(x.shape).to(device)\n",
    "#         elif mode==MODE_D:\n",
    "#             #z = t.zero_(z)#, device=device)\n",
    "#             z = t.zeros(z.shape).to(device)\n",
    "#         else:\n",
    "#             raise\n",
    "        \n",
    "        #speard z into an image and have con1a work on it\n",
    "        \n",
    "        if mode == MODE_G\n",
    "        x1 = t.nn.functional.leaky_relu(self.fc1a(z.squeeze()).view(-1, nc, image_size, image_size), negative_slope=0.2)\n",
    "        x1 = t.nn.functional.leaky_relu(self.bn1a(self.conv1a(x1)), negative_slope=0.2)\n",
    "        x = t.nn.functional.leaky_relu(self.bn2(self.conv2(x)), negative_slope=0.2)\n",
    "        #print (x.shape)\n",
    "        x = t.nn.functional.leaky_relu(self.bn3(self.conv3(x)), negative_slope=0.2)\n",
    "\n",
    "        x2 = t.nn.functional.leaky_relu(self.bn1b(self.conv1b(x)), negative_slope=0.2)\n",
    "        x = t.nn.functional.leaky_relu(self.bn4(self.conv4(x)), negative_slope=0.2)\n",
    "        #print (x.shape)\n",
    "        x = t.nn.functional.leaky_relu(self.bn5(self.conv5(x)), negative_slope=0.2)\n",
    "        #print (x.shape)        \n",
    "        x = t.nn.functional.leaky_relu(self.bn6(self.conv6(x)), negative_slope=0.2)\n",
    "        #print (x.shape)\n",
    "        fake = t.tanh(self.conv7(x)) #--> this part is to generate fake images\n",
    "        \n",
    "        #print (x.shape)\n",
    "        x = self.aapool2(x).squeeze()\n",
    "        #print (x.shape)\n",
    "        x = t.nn.functional.leaky_relu(self.fc2(x), negative_slope=0.2)\n",
    "        pred = t.sigmoid(self.fc3(x))\n",
    "        return fake, pred\n",
    "    \n",
    "class DiscriminatorV2(Generator):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__(ngpu)\n",
    "        self.aapool2 = t.nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc2 = t.nn.Linear(ngf * 2, 1024)\n",
    "        self.fc3 = t.nn.Linear(1024, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #print (x.shape)\n",
    "        if x.shape[1] == nz: #if input is a latent vector, convert it into an image\n",
    "            x = t.nn.functional.leaky_relu(self.fc1(x.squeeze()).view(-1, nc, image_size, image_size), negative_slope=0.2)\n",
    "        #print (x.shape)\n",
    "        x = t.nn.functional.leaky_relu(self.bn1(self.conv1(x)), negative_slope=0.2)\n",
    "        #print (x.shape)\n",
    "        x = t.nn.functional.leaky_relu(self.bn2(self.conv2(x)), negative_slope=0.2)\n",
    "        #print (x.shape)\n",
    "        x = t.nn.functional.leaky_relu(self.bn3(self.conv3(x)), negative_slope=0.2)\n",
    "        #print (x.shape)\n",
    "        x = t.nn.functional.leaky_relu(self.bn4(self.conv4(x)), negative_slope=0.2)\n",
    "        #print (x.shape)\n",
    "        x = t.nn.functional.leaky_relu(self.bn5(self.conv5(x)), negative_slope=0.2)\n",
    "        #print (x.shape)        \n",
    "        fake = t.tanh(self.conv6(x)) #--> this part is to generate fake images\n",
    "        \n",
    "        #print (x.shape)\n",
    "        x = self.aapool2(x).squeeze()\n",
    "        #print (x.shape)\n",
    "        x = t.nn.functional.leaky_relu(self.fc2(x), negative_slope=0.2)\n",
    "        pred = t.sigmoid(self.fc3(x))\n",
    "        \n",
    "        return fake, pred\n",
    "\n",
    "class DiscriminatorV1(t.nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.conv1 = t.nn.Conv2d(nc, ndf, 4, 2, 1, bias=False)\n",
    "        self.bn1 = t.nn.BatchNorm2d(ndf * 1)\n",
    "\n",
    "        self.conv2 = t.nn.Conv2d(ndf * 1, ndf * 2, 4, 2, 1, bias=False)\n",
    "        self.bn2 = t.nn.BatchNorm2d(ndf * 2)\n",
    "        self.conv3 = t.nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False)\n",
    "        self.bn3 = t.nn.BatchNorm2d(ndf * 4)\n",
    "        self.conv4 = t.nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False)\n",
    "        self.bn4 = t.nn.BatchNorm2d(ndf * 8)\n",
    "        self.conv5 = t.nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = t.nn.functional.leaky_relu(self.bn1(self.conv1(x)), negative_slope=0.2)\n",
    "        x = t.nn.functional.leaky_relu(self.bn2(self.conv2(x)), negative_slope=0.2)\n",
    "        x = t.nn.functional.leaky_relu(self.bn3(self.conv3(x)), negative_slope=0.2)\n",
    "        x = t.nn.functional.leaky_relu(self.bn4(self.conv4(x)), negative_slope=0.2)\n",
    "        x = t.sigmoid(self.conv5(x))\n",
    "        return x\n",
    "\n",
    "class DiscriminatorV0(t.nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = t.nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            t.nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            t.nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            t.nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            t.nn.BatchNorm2d(ndf * 2),\n",
    "            t.nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            t.nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            t.nn.BatchNorm2d(ndf * 4),\n",
    "            t.nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            t.nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            t.nn.BatchNorm2d(ndf * 8),\n",
    "            t.nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            t.nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            t.nn.S ()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"DISCRIMINATOR #1\")\n",
    "netd1 = Discriminator(ngpu).to(device)\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netd1 = t.nn.DataParallel(netd1, list(range(ngpu)))\n",
    "netd1.apply(weight_init)\n",
    "print(netd1)\n",
    "\n",
    "print (\"DISCRIMINATOR #2\")\n",
    "netd2 = Discriminator(ngpu).to(device)\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netd2 = t.nn.DataParallel(netd2, list(range(ngpu)))\n",
    "netd1.apply(weight_init)\n",
    "print(netd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = t.nn.BCELoss()\n",
    "#criterion = t.nn.BCEWithLogitsLoss()\n",
    "\n",
    "fixed_noise = t.randn(64, nz, 1, 1, device=device)\n",
    "fixed_image = t.randn(64, nc, image_size, image_size, device=device)\n",
    "#fixed_noise = create_noise(nz)\n",
    "\n",
    "label_real = 1\n",
    "label_fake = 0\n",
    "\n",
    "optimizer_d1 = optim.Adam(netd1.parameters(), lr=lr, betas=(beta1, .999))\n",
    "optimizer_d2 = optim.Adam(netd2.parameters(), lr=lr, betas=(beta1, .999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_list = []\n",
    "g_loss = []\n",
    "d_loss = []\n",
    "iters = 0\n",
    "\n",
    "print ('Start training')\n",
    "\n",
    "    \n",
    "def train_d(netg, netd, data, optimizer_g, optimizer_d):\n",
    "    netd.zero_grad()\n",
    "    real_cpu = data[0].to(device)\n",
    "    b_size = real_cpu.size(0)\n",
    "    label = t.full((b_size,), label_real, dtype=t.float, device=device) #create a vector [label_real, ...] equal to batch size\n",
    "    \n",
    "    #netd discriminate real image\n",
    "    noised = t.randn(b_size, nz, 1, 1, device=device)\n",
    "    faked, output = netd(real_cpu, noised, mode=MODE_D)\n",
    "    output = output.view(-1)\n",
    "    err_d_real = criterion(output, label)\n",
    "    err_d_real.backward()\n",
    "    D_x = output.mean().item() #why mean here? mean over all batch values?\n",
    "\n",
    "    #netd discriminates fake image\n",
    "    noise = t.randn(b_size, nz, 1, 1, device=device)\n",
    "    fake, outputg = netg(real_cpu, noise, mode=MODE_G)\n",
    "    label.fill_(label_fake) #change the gt label, no be [label_fake, ...] instead\n",
    "    faked, output = netd(fake.detach(), noise, mode=MODE_D)\n",
    "    output = output.view(-1)\n",
    "    err_d_fake = criterion(output, label)\n",
    "    err_d_fake.backward()\n",
    "    D_G_z1 = output.mean().item() #D(G(z)) over batch of z\n",
    "    err_d = err_d_real + err_d_fake\n",
    "        \n",
    "    optimizer_d.step()\n",
    "    \n",
    "    #netd discriminates fake image, and trand netg\n",
    "    netg.zero_grad()\n",
    "    #noise = t.randn(b_size, nz, 1, 1, device=device)\n",
    "    label.fill_(label_real)\n",
    "    faked, output = netd(fake, noise, mode=MODE_D)\n",
    "    output = output.view(-1)\n",
    "    err_g = criterion(output, label)\n",
    "    err_g.backward()\n",
    "    D_G_z2 = output.mean().item()\n",
    "        \n",
    "    optimizer_g.step()\n",
    "    \n",
    "    return err_d, err_g, D_x, D_G_z1, D_G_z2\n",
    "    \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        err_d, err_g, D_x, D_G_z1, D_G_z2 = train_d(netd1, netd2, data, optimizer_d1, optimizer_d2)\n",
    "        train_d(netd2, netd1, data, optimizer_d2, optimizer_d1)\n",
    "        \n",
    "        if i%50 == 0:\n",
    "            print (f'[{epoch}/{num_epochs}][{i}/{len(dataloader)}] \\t Loss_D: {err_d.item():.4f}, Loss_G: {err_g.item():.4f} \\t D(x): {D_x} \\t D(G(z)): {D_G_z1:.4f} / {D_G_z2:.4f}')\n",
    "        \n",
    "        g_loss.append(err_g.item())\n",
    "        d_loss.append(err_d.item())\n",
    "        \n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs - 1) and (i == len(dataloader) - 1)):\n",
    "            with t.no_grad():\n",
    "                fakeg, outputg = netd1(fixed_image, fixed_noise, MODE_G)\n",
    "                fake = fakeg.detach().cpu()\n",
    "            img_list.append(tv.utils.make_grid(fake, padding=2, normalize=True))    \n",
    "            \n",
    "        iters += 1#fine grain iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('generator & discriminator loss')\n",
    "plt.plot(g_loss, label='G')\n",
    "plt.plot(d_loss, label='D')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.axis('off')\n",
    "plt.title('real images')\n",
    "plt.imshow(np.transpose(tv.utils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(), (1, 2, 0)))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.axis('off')\n",
    "plt.title('fake images')\n",
    "plt.imshow(np.transpose(img_list[-1], (1, 2, 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape, label.shape, real_cpu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_pytorch_python3)",
   "language": "python",
   "name": "env_pytorch_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
